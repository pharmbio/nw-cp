[["index.html", "Network analysis approach using morphological profiling of chemical perturbation Intersecting Graph Representation Learning and Cell Profiling: A Novel Approach to Analyzing Complex Biomedical Data Aim What you would find here", " Network analysis approach using morphological profiling of chemical perturbation Nima Chamyani 2023-05-11 Intersecting Graph Representation Learning and Cell Profiling: A Novel Approach to Analyzing Complex Biomedical Data Uppsala Universitet Department of Pharmaceutical Biosciences This is a master’s project documentation for pharmaceutical modeling program at Uppsala University. Pharmaceutical Bioinformatics Research Group Nima Chamyani Aim An innovative and powerful method of analyzing complex biomedical data can be found in the intersection of graph representation learning and cell profiling. The goal of our research is to unlock new insights into how complex relation between chemical compounds, cellular phenotypes, and biological entities like proteins and biological pathways can be modeled for different purposes, ultimately facilitating the discovery and development of new drugs. What you would find here This documentation provides an in-depth account of the procedures and methodologies used within the scope of this research project, including a thorough and detailed explanation of the implemented codes and their deployment. The result and discussion is also included at the end of the documentation. "],["intro.html", "1 Introduction 1.1 Graphs 1.2 Graph representation learning 1.3 Cell profiling 1.4 Why graph representation learning?", " 1 Introduction Recent years have seen the development of graph representation learning and cell profiling, which have opened up new avenues for understanding biological systems and identifying new therapeutic strategies. By integrating these cutting-edge techniques, we can harness the power of high-dimensional data within the context of network medicine, providing valuable insights into how chemical compounds, diseases, proteins, and genes are related. This research project explores the potential of graph representation learning to analyze cell profiling data and reveal hidden connections that could help drive drug discovery. There has been substantial progress in graph representation learning in recent years, a subfield of machine learning that deals with processing and analyzing graphs or networks. For a wide range of applications, including social networks, molecular interactions, and recommendation systems, graph neural networks (GNNs) and other graph-based machine learning algorithms have been developed to model complex relationships between nodes and edges. By predicting protein-protein interactions, understanding the association between genes and diseases, and discovering new drug targets, graph representation learning has shown promise in the context of biological and biomedical data. Through the analysis of underlying mechanisms in health and disease, these techniques have the potential to uncover novel insights and deepen our understanding of the underlying structure and relationships within complex biological networks. Meanwhile, cell profiling has emerged as a powerful high-throughput approach to quantifying the phenotypic features of cells under various conditions, such as genetic alterations or chemical perturbations. In drug discovery, functional genomics, and personalized medicine, cell profiling can be used to evaluate cellular response to different stimuli rapidly and accurately. Researchers can analyze cell images to extract meaningful features, enabling them to characterize cellular phenotypes and identify biomarkers associated with specific diseases or treatments using advanced imaging techniques and computational methods. As a result of combining graph representation learning with cell profiling, we offer an innovative way to analyze cellular response data that contains a wealth of information. Using this integration, we will be able to construct an integrated network of chemical perturbations, cellular phenotypes, and other biological entities. Graph representation learning techniques will then be employed to identify hidden relationships and patterns within the network, potentially uncovering new drug targets, combinations of therapies, or mechanisms of action that may not have been apparent through traditional analysis methods. GNNs have been demonstrated to be effective in analyzing morphological profiles of cellular responses to drug perturbations in studies such as Xie et al. (2021). An innovative and powerful method of analyzing complex biomedical data can be found in the intersection of graph representation learning and cell profiling. The goal of our research is to unlock new insights into how chemical compounds, cellular phenotypes, and biological entities interact, ultimately facilitating the discovery and development of new drugs. 1.1 Graphs The concept of networks has become ubiquitous in biology and medicine. They can represent molecular interaction, protein-protein relations, gene-disease connection and etc. Biomedical knowledge and biological systems represented as graphs are more useful due to the multitude of biological entities and associations they hold. Understandably this became a highly effective motivation for the development of deep learning techniques that can be performed on networks. These machine learning methods are generally categorised under graph representation learning. Also, finding architectural features of molecular interaction in the cell follows the same pattern that we can observe in different complex networks allowed the expertise from well-understood non-biological systems like the World Wide Web or social sciences, to be useful in characterizing relationships that govern the biological systems too. Additionally, networks are often multi-modal and diverse in terms of data and this not only enhances the performance of predictive deep-learning models but also enables their broad generalization. Besides, it makes the models easier to interpret and understand(li2022graph?). Graphs (networks) map out different biological entities to a set of nodes and links. Nodes or vertices are the components of the biological system like proteins, genes, drug compounds or any other entities. Links or edges are the interaction or relation between two components (nodes). The structure of the graph can be explained by these topological units. Together they will define the dynamics of the network(barabasi2004network?). Graph descriptors categories Networks can be mathematically represented and analyzed through their topology and dynamics. Nodes (N) represent the components of a system, and links or edges (L) represent the interactions between those components. There are several size-dependent descriptors for networks, such as the degree (k), path length (l), and clustering coefficient (C), which help to quantify the connectivity, navigability, and local interconnectedness of networks, respectively. Additionally, size-independent descriptors like the degree distribution (P(k)) and the average clustering coefficient with k links (C(k)) offer insights into the network’s hierarchical character (Graph descriptors categories). Aside from these units and descriptors, there are often hierarchical modules in the complex graphs. For instance, a subset of nodes that are connected to each other in a specific wiring diagram is called a subgraph and those subgraphs that occur more frequently in the graph are called motifs. Motifs and subgraphs can be used to define a highly interconnected group of nodes with relatively distinct functions or behaviour which seems to be a property of all complex networks(Stamile2021Jun?). Network architectures also can be categorized into various models, such as scale-free, random, and hierarchical networks. Scale-free networks are highly non-uniform, with a small number of nodes having many links and most nodes having few links. These networks exhibit a power-law degree distribution, with a degree exponent γ. Hierarchical networks, on the other hand, are characterized by their modularity, local clustering, and scale-free topology, and can be described by iterative clustering. Random networks are more uniform, with links placed randomly and a Poisson degree distribution. The widely-used machine-learning approach for graphs is representation learning. There are several types of graphs that can be used for representation learning. Graphs can be simple, weighted, multi-modal, knowledge-based, multi-layer or temporal. Nodes and edges can also be spatial elements of a multi-dimensional space e.g., a spatial representation of cell interactions in euclidean space or a 3D point cloud of the molecule’s atomic coordinates. All in all, the aim of ML is to realize the essence of graphs. The first step usually is to transform nodes and edges to a lower vectorial space which is called embedding. After embedding the complex structure of the graph into lower space, different types of machine-learning techniques can be performed on the data(zhang2018network?). Summary on network basics Complex networks exhibit several interesting features, such as the small-world effect, disassortativity, and hierarchical modularity. The small-world effect implies that any two nodes can be connected with a path of only a few links. Disassortativity refers to the tendency of highly connected nodes (hubs) to avoid linking directly to each other, while nodes of low degree are more likely to connect with nodes of high degree. Hierarchical modularity is evident in the presence of motifs (subgraphs occurring more frequently in the network), motif clusters, and modules (clusters of highly interconnected nodes with distinct functions or behaviors). Graph representation learning has become a leading machine learning approach for networks, allowing for the principles of networks to be realized in a compact vectorial space, or embeddings. This technique involves specifying nonlinear transformation functions that map nodes to points in the embedding space. Various types of graphs can be used for representation learning, including simple, weighted, attributed, multimodal, heterogeneous, knowledge graphs, multi-layer graphs, temporal graphs, and spatial graphs. These graph types can be combined to create new objects, such as multi-layer spatial or multimodal temporal graphs. In conclusion, network science and graph representation learning have become increasingly important tools for studying complex systems. By leveraging the power of these techniques, researchers can gain new insights into the structure and dynamics of complex networks in various fields, leading to a deeper understanding of the world around us. 1.2 Graph representation learning For understanding complex networks and extracting meaningful information from them, graph representation learning has emerged as a powerful approach. To achieve various downstream tasks such as node classification, link prediction, and graph classification, graph data must be learned and encoded to learn and encode their inherent structure and features. We will provide a brief overview of graph representation learning concepts and methods, emphasizing their relevance in various applications, in this section. Graph embedding methods aim to represent nodes, edges, or entire graphs as continuous low-dimensional vectors while preserving the underlying graph structure. These embeddings can be derived using both unsupervised and supervised learning paradigms. Through the use of graph connectivity patterns, unsupervised methods like DeepWalk, node2vec, and LINE learn latent feature representations, while supervised methods like GraphSAGE and Graph Convolutional Networks (GCN) utilize node features and labels to guide the learning process. In the realm of graph neural networks (GNNs), message-passing techniques like GGSNN and MPNN provide a powerful way to learn node and edge representations by propagating and aggregating information from local neighborhoods. These methods have been extended to non-Euclidean domains with frameworks like ChebyNet, GAT, and MoNet, which capture the intrinsic geometry and topology of the graph. An important aspect of graph representation learning is the development of autoencoders, such as SDNE, DNGR, and VGAE, which are able to learn low-dimensional embeddings without supervision based on the graph’s structure. To reconstruct the original graph from the learned embeddings, matrix factorization, skip-gram models, and other techniques are often employed. Autoencoders can also be combined with graph regularization and manifold learning methods, such as Isomap, MDS, and LLE, to further improve the quality of the learned representations. For generating graphs with desirable properties or learning latent graph spaces, graph generation models have been developed, such as GCPN, JT-VAE, and GraphRNN. In applications like drug discovery, generating new chemical structures with specific characteristics is of paramount importance, which makes these models particularly useful. Graph representation learning methods There have been significant advancements in drug discovery, genomics, and proteomics due to machine learning applied to biomedical graphs. Different real-world problems have been studied and applied to canonical graph prediction tasks, such as node classification, link prediction, and graph classification. Furthermore, recent advances in latent graph learning have shown promise in uncovering hidden patterns within complex networks. It has already shown that graph representation learning has the potential to transform our understanding of complex networks as it is a rapidly evolving research area. Researchers continue to develop innovative approaches for analyzing and modeling graph data by combining graph theory, network diffusion, topological data analysis, and manifold learning. The vast and complex world of graphs will become even more meaningful as our knowledge in this field advances. 1.3 Cell profiling Cell painting and cell profiling are advanced high-content imaging and analysis techniques used in cell biology to comprehensively characterize cellular phenotypes. These methods combine high-throughput microscopy with computational analysis to simultaneously investigate multiple cellular features, enabling the study of complex biological systems and the discovery of novel drug targets or biomarkers. his is particularly important in distinguishing the healthy and diseased states of the cell. Studies showed the great capability of this method in disease recognition or looking for many treatments effect within a single day. The output of this high-throughput microscopy is images which by itself can be an excellent source of unbiased information for analysis. There have been numerous studies that used deep learning on these images and resulted in models with unmatched power for identifying disease-specific phenotypes in clinical diagnostics and chemical lead hopping in drug discovery and so on. Additionally, specific phenotype features of any perturbation can be extracted to be used for data mining and profile analysis(Rietdijk et al. 2022). With cell painting, different type of dyes are used to stain cells and target specific subcellular compartments or structures. By multiplexing several cellular features, including organelles, cytoskeletons, and nuclear compartments, several cellular features can be visualized simultaneously. By imaging stained cells with high-throughput microscopy, numerous morphological, textural, and intensity-based features are captured, resulting in a wealth of quantitative data. Subsequently, cell profiling is employed to analyze the acquired images. This process leverages computational algorithms, such as machine learning and deep learning, to extract quantitative information from the images. The resulting high-dimensional data, often referred to as “features” or “phenotypic fingerprints”, is used to classify cells, identify functional relationships, or reveal previously unrecognized patterns. These methods have been used to address various biological questions. Bray et al. (2016), for example, utilized these techniques to create a morphological map of human cells exposed to a variety of bioactive compounds. The results of their research have led to the identification of novel compound-target interactions, demonstrated the potential of cell painting as a tool for high-throughput drug discovery, and provided a comprehensive resource for the scientific community. In addition, cell painting and cell profiling have allowed scientists to dissect signaling pathways, elucidate gene function, and understand disease mechanisms. Aside from uncovering novel gene functions and dissecting cellular networks, researchers have used these methods to study how genetic or chemical perturbations affect cell phenotypes. Drug discovery, precision medicine, and basic research can all benefit from cell painting and cell profiling. In addition to revealing novel drug targets, these techniques can also identify potential therapeutic compounds and elucidate the mechanisms underlying disease pathology by providing a comprehensive, systems-level view of cellular phenotypes. The high-throughput nature of these methods also allows rapid screening of vast compound libraries or genetic perturbations, which accelerates scientific discovery and facilitates the development of personalized therapeutics. Our understanding of cellular biology has been greatly enriched by the use of cell painting and cell profiling, powerful, high-content imaging and analysis techniques. These methods have aided in the discovery of new drugs, precision medicine, and basic research by offering a comprehensive view of cellular phenotypes, and they remain an essential tool in modern cell biology. 1.4 Why graph representation learning? When dealing with data structured as graphs or networks, such as cell painting data, graph representation learning provides a powerful alternative to conventional deep learning techniques. The cell painting technique uses dyes to stain multiple cell compartments to capture rich morphological information. The inherent complexity and relational nature of cell painting data can be more effectively modeled using graph representation learning techniques and here’s why. Graph representation learning is explicitly designed to model complex relationships between nodes (entities) and edges (connections) within a graph, whereas deep learning techniques, such as feedforward neural networks and convolutional neural networks (CNNs), use fixed-size inputs. As graph representation learning retains the inherent structure and connections in the data, it can better capture dependencies and interactions between entities, improving prediction performance. Also, the graph representation learning algorithm can handle a variety of input data types. Learning can be enhanced by including rich, multimodal information to gain a more comprehensive understanding of the underlying relationships. By doing so, we can include more information about a compound, such as its connection to proteins or biological pathways. Furthermore, graph representation learning algorithms are invariant to isomorphism. They can produce consistent node embeddings and predictions regardless of the order or label of the nodes. For graph-structured data, this property is especially important, as it ensures that learned representations and models are resilient to arbitrary relabeling or reordering. Since conventional deep learning techniques are sensitive to the order or arrangement of input features, graph data may limit their applicability and performance. Finnaly , the sparse and local nature of graph data can be exploited to make graph representation learning and inference efficient. In GraphSAGE and GraphConvolutional Networks (GCNs), for example, localized convolutional operations are applied to graphs, enabling efficient neighborhood information aggregation and scalable learning of large-scale graphs. In contrast, conventional deep learning techniques may require dense representations or extensive memory resources, particularly when dealing with high-dimensional input data. References "],["methods.html", "2 Methods and materials 2.1 Data acquisition, softwares and libraries 2.2 Data Preprocessing", " 2 Methods and materials A diverse set of computational tools and methodologies were employed in this study to analyze and interpret complex biomedical data. 2.1 Data acquisition, softwares and libraries The data used in this study consists of complex biomedical data, encompassing information related to more than 5000 chemical compounds, their related cellular phenotypes in infected cells with COVID-19, and various biological entities like proteins and pathways in humans. High-dimensional cell profiling data was collected from the Pharmaceutical Bioinformatics Research Group at Uppsala University. The data encompassed various cellular phenotypes under chemical perturbations. The data was then preprocessed and normalized to ensure uniformity and mitigate the effects of batch variations and other noise. Additionally, chemical compound information, including molecular structure and other biological entities like proteins and biological pathways, was collected from several relevant databases. Initial data preprocessing was performed using the polars and pandas Python packages for data manipulation. Any inconsistencies, outliers, and missing values were identified and handled appropriately to ensure data integrity. The main analytical technique used in this study is graph representation learning. The analysis was predominantly performed in Python (version 3.8), with a variety of libraries. PyTorch (version 1.13.1+cu116), a deep learning framework that allows for flexible and efficient model building and training, was used. For the implementation of graph neural networks (GNNs), both PyTorch Geometric (version 2.2.0) and Deep Graph Library (DGL, version 1.0.1+cu116) were utilized. These libraries are extensions of PyTorch designed specifically for graph representation learning, supporting a variety of GNN models and providing efficient and scalable implementations. DeepChem (version 2.7.1) was utilized for pre-processing and some featurization of chemical compounds. Physico-chemical featurization were performed by RDKit and Mordred library. TorchDrug (version 0.2.0.post1), an emerging toolkit for drug discovery, was used to create network architectures for drug generation tasks. Chemoinformatics operations, such as converting chemical structures to molecular graphs, were carried out using RDKit (version 2022.09.5). The Scikit-learn library was employed for general machine learning tasks, including model evaluation and hyperparameter tuning. For the visualization of the data and results, Plotly, a graphing library that enables the creation of interactive and high-quality visualizations, was primarily used. All statistical analyses were performed using the Scikit-learn library in Python and some specific analyses were conducted in the R programming language. The analyses included standard measures of model performance, as well as more complex analyses like feature importance ranking. By employing this combination of methods and materials, the intersection of graph representation learning and cell profiling was explored, revealing novel insights into the interactions between chemical compounds, cellular phenotypes, and biological entities. This integrated approach facilitated the discovery of new relationships and potential drug targets that may not have been apparent through traditional analysis methods. 2.2 Data Preprocessing 2.2.1 COVID-19 Cell profilling Data In this data processing, the dataset containing phenotype features and metadata for multiple images was prepared for analysis. The dataset was cleaned by dropping possible empty features and features with the same value. All the numeric columns were extracted, and the phenotype features were identified and extracted from these columns. Features with low variability were removed, based on their standard deviation. This dataset represents an extensive collection of phenotype features extracted from images, along with associated metadata. Each row in the dataset corresponds to a single image, with each image associated with a specific site within a well. There are 9 sites (numbered 1-9) within each well, and approximately 350-360 wells within each plate, with a maximum of 384 wells per plate. The dataset encompasses 24 plates. ImageID ~ 2000 phenotype features PlateID Well Site Plate Plate_Well batch_id pertType cmpd_conc Flag Count_nuclei Batch nr Compound ID selected_mechanism P03-L2_B03_1 ……….. values ………. P03-L2 B03 1 03-L2 specs935-plate03-L2_B03 BJ1894547 trt 10.0 0 109.0 BJ1894547 CBK042132 estrogen receptor alpha modulator P03-L2_B03_2 ……….. values ………. P03-L2 B03 2 03-L2 specs935-plate03-L2_B03 BJ1894547 trt 10.0 0 121.0 BJ1894547 CBK042132 estrogen receptor alpha modulator . . . |54366 rows, 2140 columns| During the preparation phase, the first step involved dropping empty features, i.e., columns with no values or with a standard deviation (SD) of 0. The dataset was then segregated into numeric columns, further filtered down to ‘phenotype features’ and ‘metadata’. The ‘phenotype features’ included numeric columns excluding those with certain strings such as ‘Metadata’, ‘Number’, ‘Outlier’, ‘ImageQuality’, ‘cmpd_conc’, ‘Total’, ‘Flag’ and ‘Site’. The difference between the number of numeric columns and the number of phenotype features gives the number of ‘metadata’. numeric_columns = list() for a in df.columns: if (df.dtypes[a] &#39;float64&#39;) | (df.dtypes[a] &#39;int64&#39;) : numeric_columns.append(a) feature_columns = [fc for fc in numeric_columns if (&#39;Metadata&#39; not in fc) &amp; (&#39;Number&#39; not in fc) &amp; (&#39;Outlier&#39; not in fc) &amp; (&#39;ImageQuality&#39; not in fc) &amp; (&#39;cmpd_conc&#39; not in fc) &amp; (&#39;Total&#39; not in fc) &amp; (&#39;Flag&#39; not in fc) &amp; (&#39;Site&#39; not in fc) ] The preparation phase also involved removing any features with missing values and those with an SD less than 0.0001. X = df.loc[:, feature_columns] X.dropna(axis=1, inplace=True) X = X.loc[:, (X.std() &gt; 0.0001) ] 2.2.1.1 Normalization In the normalization phase, the features were normalized based on DMSO (control), both in a general approach (without plate separation) and separated by plates. In the general approach, the median of all data points was taken at once, and the Z-scores were calculated using the median and median absolute deviations (MADs) for DMSO. \\[Z = \\frac{X - DMSO_{median}}{|X_{dmso} - DMSO_{median}|_{median}}\\] where \\(X\\) is observed feature value, \\(DMSO_{median}\\) is median of DMSO and \\(X_{dmso}\\) is observed feature value of DMSO. dfDMSO = df[df[&#39;batch_id&#39;] == &#39;[dmso]&#39;] dfDMSO_Medians = dfDMSO[phenotype_features].median() dfDMSO_MADs = (dfDMSO[phenotype_features] - dfDMSO[phenotype_features].median()).abs().median() df_Zscores = df[phenotype_features].copy() df_Zscores = (df[phenotype_features] - dfDMSO_Medians[phenotype_features])/dfDMSO_MADs[phenotype_features] df_Zscores.clip(lower=-10, upper=10, inplace=True) In the plate separated approach, the same process was applied. However, it was done first by finding local median values for DMSO in each plate and normalizing the measurements in the same plate based on their respective DMSO medians. df_Zscores_by_plate = pd.DataFrame() for plate in plates: plate_data = df[df[&#39;Plate&#39;] == plate] df_DMSO = plate_data[plate_data[&#39;batch_id&#39;] == &#39;[dmso]&#39;] df_DMSO_medians = df_DMSO[phenotype_features].median() df_DMSO_MADs = (df_DMSO[phenotype_features] - df_DMSO[phenotype_features].median()).abs().median() Zscores = (df[df[&#39;Plate&#39;] == plate][phenotype_features] - df_DMSO_medians[phenotype_features])/df_DMSO_MADs[phenotype_features] df_Zscores_by_plate = pd.concat([df_Zscores_by_plate, Zscores]) df_Zscores_by_plate 2.2.1.2 Dimensionality Reduction In this part, several key steps were undertaken to reduce the dimensionality of the data, select the most informative features, and visualize the structure and variability of the data using Principal Component Analysis (PCA). PCA was applied to the dataset to identify the key direction or “a component” that describes most of the data variability. This process helps to transform the original dataset into an updated one where each data point is represented in terms of this component. The PCA algorithm also provides the loadings, or the contribution of each original feature to each principal component. from pca import pca import matplotlib.pyplot as plt X = covid_df.loc[(covid_df[&#39;label&#39;] == &#39;DMSO&#39;) | (covid_df[&#39;label&#39;] == &#39;Uninfected&#39;) | (covid_df[&#39;label&#39;] == &#39;Remdesivir&#39;)][features].values row_label = covid_df.loc[(covid_df[&#39;label&#39;] == &#39;DMSO&#39;) | (covid_df[&#39;label&#39;] == &#39;Uninfected&#39;) | (covid_df[&#39;label&#39;] == &#39;Remdesivir&#39;)][&#39;label&#39;] PCA_model = pca(n_components=10, detect_outliers=[&#39;ht2&#39;, &#39;spe&#39;]) results = PCA_model.fit_transform(X, col_labels=features, row_labels=row_label) PCA_model.plot(figsize=(12, 6)) Using outlier detection method like the Hotelling T2 test and the squared prediction error (SPE/DmodX) just one outlier were found in the data which was decided to remain in data. fig, axes = plt.subplots(ncols=2, figsize=(17,8)) ax1 = PCA_model.scatter(SPE=True, hotellingt2=True, cmap=&#39;tab10&#39;, ax=axes[0]) ax2 = PCA_model.biplot(SPE=True, hotellingt2=True, fontdict={&#39;size&#39;: 8}, cmap=&#39;tab10&#39;, PC=[0,1,2], ax=axes[1]) plt.show() The loading values obtained from PCA were subsequently utilized as input for a k-means clustering algorithm, enabling the clustering of features according to their loadings. The idea is to find the features that provide same information and cluster them together. The process begins with the execution of PCA, which is then followed by the deployment of k-means clustering on the PCA loadings. This arrangement allows features to be clustered based on their loadings. This can be construed as their significance or contribution to the data variance then a new feature is calculated for every cluster. This feature represents the average of all features contained within that specific cluster. def feature_reducer(df, feature_list, loading_dim=32, feat_output_dim=32): import pandas as pd from sklearn import preprocessing from sklearn.decomposition import PCA from sklearn.cluster import KMeans df_dsmo_uninfected_remi = df.loc[(df[&#39;label&#39;] == &#39;DMSO&#39;) | (df[&#39;label&#39;] == &#39;Uninfected&#39;) | (df[&#39;label&#39;] == &#39;Remdesivir&#39;)][feature_list] X = df_dsmo_uninfected_remi.values pca = PCA() pca.fit(X) loadings = pca.components_ loading_data = pd.DataFrame(loadings[:loading_dim]).T.values # Perform k-means clustering kmeans = KMeans(n_clusters=feat_output_dim, random_state=42, n_init=100).fit(loading_data) # Get cluster assignments for each point labels = kmeans.labels_ for i in range(feat_output_dim): exec(f&#39;f_{i+1}&#39; + f&#39;= loading_data[labels == {i}]&#39;) f = pd.DataFrame({&#39;features&#39; : df_dsmo_uninfected_remi.columns.values, &#39;cluster&#39;: labels}).groupby(&quot;cluster&quot;).agg(list) column_list = list(df.columns) for feat in feature_list: column_list.remove(feat) new_df = df.loc[:,column_list].copy() # new_df = df.loc[:,list(set(df.columns) - set(feature_list))].copy() for i, f_list in enumerate(f[&#39;features&#39;]): new_df[f&#39;f{i+1}&#39;] = df[f_list].apply(lambda x: x.mean() , axis=1) return new_df, f In another approach to reduce the dimension, each feature’s ability to differentiate between the classes was evaluated by calculating the area of the triangle formed by the centroids of the classes in the feature space when we just use that specific feature and one highly related parameter to differentiate classes (for instance feature vs. number of nuclei) to plot all the points. For this, the centroids of the three distinct categories (‘Compound’, ‘Uninfected’, ‘Remdesivir’) for each feature ~ number of nuclei plot have been calculated. The area of the triangle formed by these centroids and the distance between them has been computed. This quantifies the separation between the three categories using each feature and helps to find the most descriptive feature. Features resulting in larger triangle areas were considered more informative. import math import numpy as np def area_of_triangle(p1, p2, p3): # Calculate the length of each side of the triangle a = math.sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2) b = math.sqrt((p3[0] - p2[0])**2 + (p3[1] - p2[1])**2) c = math.sqrt((p3[0] - p1[0])**2 + (p3[1] - p1[1])**2) # Calculate the semiperimeter of the triangle s = (a + b + c) / 2 # Calculate the area using Heron&#39;s formula area = math.sqrt(s * (s - a) * (s - b) * (s - c)) return area def distance_between_centroids(centroid1, centroid2): # Calculate the distance using the distance formula distance = np.sqrt((centroid2[0] - centroid1[0])**2 + (centroid2[1] - centroid1[1])**2) return distance scaler = preprocessing.StandardScaler(with_mean=True, with_std=True) scaled_df = covid_df.copy() scaled_df.loc[:, features] = scaler.fit_transform(scaled_df.loc[:, features]) scores = [] comp_remi = [] comp_uni = [] remi_uni = [] for feat in features[1:]: compound_coords = scaled_df.loc[scaled_df[&#39;label&#39;] == &#39;compound&#39;,[&#39;Count_nuclei&#39;,feat]].values uninfected_coords = scaled_df.loc[scaled_df[&#39;label&#39;] == &#39;Uninfected&#39;,[&#39;Count_nuclei&#39;,feat]].values remidesivir_coords = scaled_df.loc[scaled_df[&#39;label&#39;] == &#39;Remdesivir&#39;,[&#39;Count_nuclei&#39;,feat]].values compound_centroid = np.mean(compound_coords, axis=0) uninfected_centroid = np.mean(uninfected_coords, axis=0) remidesivir_centroid = np.mean(remidesivir_coords, axis=0) area = area_of_triangle(compound_centroid, uninfected_centroid, remidesivir_centroid) comp_remi_dist = distance_between_centroids(compound_centroid, remidesivir_centroid) comp_uni_dist = distance_between_centroids(compound_centroid, uninfected_centroid) remi_uni_dist = distance_between_centroids(remidesivir_centroid, uninfected_centroid) scores.append(area) comp_remi.append(comp_remi_dist) comp_uni.append(comp_uni_dist) remi_uni.append(remi_uni_dist) feat_score = pd.DataFrame({&#39;feat&#39;: features[1:], &#39;score&#39;: scores, &#39;comp_remi&#39;:comp_remi, &#39;comp_uni&#39;:comp_uni, &#39;remi_uni&#39;: remi_uni}) The features have been ranked based on the calculated area, seen as a measure of separation between the categories. The top 50 features have been selected. Between the first 50 features all annotated with MITO were removed because they bias our model. Therefore, 16 features remain as our selected features. list(feat_score.sort_values(by=[&quot;score&quot;], ascending=False).head(50)[&#39;feat&#39;]) Finally, PCA has been performed again, this time exclusively on the selected features. Link to see interactive plot The PCA results demonstrate that the 10 principal components can now explain a very high percentage (99.91%) of the variance. The first principle component improved from describing 75.1 percent of variance to 93.0 percent. This indicates that the selected features capture most of the data variability. The value of PC1 was used for regression models as the target value. 2.2.1.3 Development of a binary classification of data In this part, Kernel Density Estimation (KDE) and Empirical Confidence Regions (2d confidence intervals based on binned kernel density estimate) were utilized to inform the development of a binary classification model for identifying active and inactive compounds based on their PCA1 values. Firstly, a two-dimensional KDE was performed on the compounds’ PCA1 and PCA2 values. This KDE plot provided a comprehensive understanding of the distribution of data points in the PCA space. It highlighted two distinct clusters corresponding to active and inactive compounds. The KDE plot was further enhanced by overlaying empirical confidence regions on it. These regions were derived from the mean and covariance of the PCA1 and PCA2 values for each cluster. Two standard deviation ellipses were used, approximating 90% confidence regions for the location of the true mean of each cluster in the PCA space. import numpy as np import seaborn as sns import matplotlib.pyplot as plt from matplotlib.patches import Ellipse mean1 = df_cluster1[[&#39;PC1&#39;, &#39;PC2&#39;]].mean() cov1 = df_cluster1[[&#39;PC1&#39;, &#39;PC2&#39;]].cov() mean2 = df_cluster2[[&#39;PC1&#39;, &#39;PC2&#39;]].mean() cov2 = df_cluster2[[&#39;PC1&#39;, &#39;PC2&#39;]].cov() fig, ax = plt.subplots(figsize=(7, 4)) # Draw the KDE plot sns.kdeplot(data=df_clust, x=&#39;PC1&#39;, y=&#39;PC2&#39;, fill=True, ax=ax) # Draw the confidence ellipses for mean, cov in [(mean1, cov1), (mean2, cov2)]: eigenvalues, eigenvectors = np.linalg.eigh(cov) order = eigenvalues.argsort()[::-1] eigenvalues, eigenvectors = eigenvalues[order], eigenvectors[:, order] vx, vy = eigenvectors[:,0] theta = np.arctan2(vy, vx) # Draw a 2*2.146 ellipse for 90 % CI ellipse = Ellipse(xy=mean, width=2*2.146**np.sqrt(eigenvalues[0]), height=2*2.146**np.sqrt(eigenvalues[1]), angle=np.degrees(theta), edgecolor=&#39;red&#39;, facecolor=&#39;none&#39;) ax.add_patch(ellipse) plt.show() The intersection of these confidence regions was then examined. The PCA1 value of 5 at this intersection was hypothesized to be an effective threshold for binary classification of compounds. Any compound with a PCA1 value greater than this threshold was classified as ‘active’, and any compound with a PCA1 value less than this threshold was classified as ‘inactive’. Importantly, this approach allowed the study to estimate a suitable classification threshold but also to visualize the uncertainty around this threshold and the potential overlap between the two classes. The method provided a data-driven way to set the classification threshold and offered insights into the inherent complexity of the data. It should be noted that the assumptions of the Gaussian distribution and independent identically distributed data inherent to this method may not hold in all cases. Therefore, the results should be interpreted with caution. Further, the use of PCA1 values alone for classification may oversimplify the problem if the active and inactive compounds differ along other principal components as well. Therefore, additional analyses are recommended to validate and refine this binary classification model. 2.2.2 Compound, Protein and Pathway Data Aggregation In the cell profiling data, the Simplified Molecular Input Line Entry System (SMILES) was incorporated to represent the chemical structure. The COVID-19 cell painting experiments that was carried out, was involved cell perturbations with over 5000 compounds. To uncover potential information regarding the protein binding capabilities and the pathways and assays in which these compounds are active, a highly recognized cross-reference annotation was necessitated. The PubChem Chemical ID (CID) serves as an exhaustive cross-reference annotation for chemicals. The initial step involved the determination of the CIDs for all the chemical compounds. These identifiers were subsequently utilized for the aggregation of additional protein and pathway data. This approach facilitated finding the activity of the compounds within the biological systems. import pubchempy as pcp chemical_smiles = list(df[&#39;smiles&#39;].values) cids = [] for smiles in chemical_smiles: try: c = pcp.get_compounds(smiles, &#39;smiles&#39;) if c: cids.append(c[0].cid}) else: print(f&#39;No compound found for SMILES: {smiles}&#39;) except Exception as e: print(f&#39;Error occurred: {e}&#39;) The COVID-19 dataset consisted of compounds screened for potential activity against SARS-CoV-2. To analyze the associations between these compounds and proteins, an auxiliary dataset, sourced from the STITCH database, was utilized. The STITCH database provides information about interactions between chemicals and proteins. These connections were then indexed and collated to create a list of compounds with and their association with proteins. Further, the connection between chemical compounds and their corresponding biological pathways was established through the following steps: Initially, assay summaries for a selection of compounds were obtained from the PubChem database. These summaries provided information about various biological assays performed on the compounds, specifically emphasizing on the target gene IDs that interacted with the compounds during these assays. Only the assays reporting an ‘Active’ outcome and a non-empty target gene ID were retained for further analysis. from io import StringIO import polars as pl import pubchempy as pcp comp_gid = pl.read_csv(&#39;data/comp_gid.tsv&#39;, separator=&#39;\\t&#39;) cids = comp_gid.select([&#39;pubchem_cid&#39;]).to_series().to_list() cid_genid_df = pl.DataFrame( schema={&#39;CID&#39;: pl.Int64, &#39;AID&#39;: pl.Int64, &#39;Target GeneID&#39;: pl.Utf8, &#39;Activity Value [uM]&#39;: pl.Float64, &#39;Assay Name&#39;: pl.Utf8} ) for cid in cids[:10]: try: csvStringIO = StringIO(pcp.get(cid, operation=&#39;assaysummary&#39;, output=&#39;CSV&#39;).decode(&quot;utf-8&quot;)) dictdf = pl.read_csv(csvStringIO, dtypes={&#39;Activity Value [uM]&#39;: pl.Float64}) ciddf = dictdf.filter( (pl.col(&quot;Activity Outcome&quot;) == &quot;Active&quot;) &amp; (pl.col(&quot;Target GeneID&quot;) != &quot;&quot;) ).unique( subset=&#39;Target GeneID&#39; ).select( [&#39;CID&#39;, &#39;AID&#39;, &#39;Target GeneID&#39;, &#39;Activity Value [uM]&#39;, &#39;Assay Name&#39;] ) except: ciddf = pl.DataFrame( schema={&#39;CID&#39;: pl.Int64, &#39;AID&#39;: pl.Int64, &#39;Target GeneID&#39;: pl.Utf8, &#39;Activity Value [uM]&#39;: pl.Utf8, &#39;Assay Name&#39;: pl.Utf8} ) cid_genid_df = cid_genid_df.vstack(ciddf) cid_genid_df.write_csv(&#39;comp_geneid.tsv&#39;, separator=&#39;\\t&#39;) Subsequently, the list of unique target gene IDs was utilized to extract associated biological pathways from the PubChem database. These pathways, sourced from the WikiPathways database, link each gene ID to one or multiple biological pathways. import polars as pl import pubchempy as pcp gene_id_list = cid_genid_df.select(pl.col(&#39;Target GeneID&#39;).cast(pl.Int64, strict=True)).unique(subset=&#39;Target GeneID&#39;, maintain_order=True).to_series().to_list() genid_wpw_df = pl.DataFrame(schema={&#39;Target GeneID&#39;: pl.Object, &#39;Wiki Pathway&#39;: pl.Utf8}) for gene_id in gene_id_list: try: ptw_list = pcp.get_json(gene_id, namespace=&#39;geneid&#39;, domain=&#39;gene&#39;, operation=&#39;pwaccs&#39;)[&#39;InformationList&#39;][&#39;Information&#39;][0][&#39;PathwayAccession&#39;] wp = [i[13:] for i in list(filter(lambda x: &#39;WikiPathways&#39; in x, ptw_list))] temp_df = pl.DataFrame({&#39;Target GeneID&#39;:gene_id, &#39;Wiki Pathway&#39;: wp}) except: temp_df = pl.DataFrame({&#39;Target GeneID&#39;:gene_id, &#39;Wiki Pathway&#39;: &#39;&#39;}) genid_wpw_df = genid_wpw_df.vstack(temp_df) genid_wpw_df.write_csv(&#39;geneid_wpw.tsv&#39;, separator=&#39;\\t&#39;) Gene ID fetched from these databases can be converted to STITCH database annotation by using BIIT API: import requests r = requests.post( url=&#39;https://biit.cs.ut.ee/gprofiler/api/convert/convert/&#39;, json={ &#39;organism&#39;:&#39;hsapiens&#39;, &#39;target&#39;:&#39;ENSP&#39;, &#39;query&#39;:gene_id_list, &#39;numeric_namespace&#39;: &#39;ENTREZGENE_ACC&#39; } ) pl.DataFrame(r.json()[&#39;result&#39;]).select([&#39;incoming&#39;, &#39;converted&#39;, &#39;name&#39;, &#39;description&#39;]) Hence, a linkage from chemical compounds to biological pathways was constructed by associating compounds with target gene IDs from assays, and then connecting these gene IDs to biological pathways. This method of data preparation facilitated the exploration of potential mechanisms of action of compounds. It also provided an understanding of the biological processes potentially influenced by these compounds. It should be noted, however, that this is a simplified representation of the actual biological interactions, which are inherently more complex. These curated lists of compounds, proteins, and pathways served as the foundation for constructing a multimodal graph. In this graph, the nodes represent compounds, proteins, and pathways while the edges depict the connections between them. 2.2.3 Featurizing the Biomedical Entities In machine learning, featurization converts biomedical entities, which are often three-dimensional entities like chemicals and proteins, into a format that can be understood and processed by algorithms. Essentially, it involves converting the structure and properties into numerical vectors. It is necessary because machine learning algorithms work with numerical data rather than understanding biological structures and properties directly. Featurization is accomplished through the application of different algorithm and ways which will be discussed in following section. 2.2.3.1 Featurizing Compounds Starting with MACCS fingerprints, these are binary representations of a molecule based on the presence or absence of 167 predefined structural fragments. MACCS fingerprints are popular due to their simplicity, interpretability, and effectiveness at capturing structural information. import deepchem as dc feat = dc.feat.MACCSKeysFingerprint() maccs_fp = feat.featurize(smiles) Morgan fingerprints, also known as circular fingerprints, are another type of molecular descriptor. They are generated by iteratively hashing the environments of atoms in a molecule. These fingerprints are characterized by their flexibility, as their radius and length can be adjusted. This allows for various levels of specificity in the representation of molecular structures. import deepchem as dc feat = dc.feat.CircularFingerprint(size=2048, radius=1) morgan_fp = feat.featurize(smiles) PubChem fingerprints are binary fingerprints consisting of 881 bits, each representing a particular chemical substructure or pattern. They were specifically designed for use with the PubChem database and provide detailed chemical structure encoding. import pubchempy as pcp cids = list(df.pubchem_cid[df[&#39;label&#39;]==&#39;compound&#39;]) bit_list = [] for cid in tqdm(cids): try: pubchem_compound =pcp.get_compounds(cid)[0] pubchem_fp = [int(bit) for bit in pubchem_compound.cactvs_fingerprint] bit_list.append(pubchem_fp) except: print(f&#39;No PubChem FP found for {cid}&#39;) bit_list.append([]) pc_fp = np.asarray(bit_list) The Mol2Vec fingerprint is inspired by the Word2Vec algorithm in Natural Language Processing. It considers molecules as sentences and SMILES as words, thus converting molecules into continuous vectors. This technique captures not only the presence of particular substructures but also their context within the molecule, providing a more nuanced representation. import deepchem as dc feat = dc.feat.Mol2VecFingerprint() m2v_fp = feat.featurize(smiles) In computational chemistry and drug discovery, molecules’ pre-treatment plays a crucial role in preparing them for machine learning applications. Molecules undergo optimization, where their 3D structure is refined and all possible conformations are explored. This step is vital as the 3D structure heavily influences various properties, such as reactivity and binding affinity. For Mordred and RDKit to compute all descriptors, we must optimize molecules and find their most energetically favorable conformations. from rdkit import Chem from rdkit.Chem import AllChem from threading import active_count num_thread = active_count() def optimize_3d(mol, method): mol = Chem.AddHs(mol) # Generate initial 3D coordinates params = AllChem.ETKDG() params.useRandomCoords=True params.maxAttempts=5000 AllChem.EmbedMolecule(mol, params) try: if method == &#39;MMFF&#39;: # optimize the 3D structure using the MMFF method (suitable for optimizing small to medium-sized molecules) AllChem.MMFFOptimizeMolecule(mol, maxIters=200, mmffVariant=&#39;MMFF94s&#39;) elif method == &#39;LOPT&#39;: # optimize the 3D structure using a combination of UFF and MMFF methods (can be used for optimizing larger and more complex molecules) # create a PyForceField object and set its parameters ff = AllChem.UFFGetMoleculeForceField(mol) ff.Initialize() ff.Minimize() AllChem.OptimizeMolecule(ff, maxIters=500) elif method == &#39;CONFOPT&#39;: # optimize each conformation using the PyForceField object (to explore the conformational space of a molecule and identify the most energetically favorable conformations) # generate 10 conformations using UFF # create a PyForceField object and set its parameters AllChem.EmbedMultipleConfs(mol, numConfs=10) ff = AllChem.UFFGetMoleculeForceField(mol) ff.Initialize() ff.Minimize() AllChem.OptimizeMoleculeConfs(mol, ff, maxIters=300, numThreads=num_thread) else: print(f&quot;method should be from {[&#39;MMFF&#39;, &#39;LOPT&#39;, &#39;CONFOPT&#39;]}&quot;) except: pass return mol RDKit descriptors include a comprehensive set of descriptors calculated directly from the molecule’s structure. These descriptors encompass a wide range of molecular properties, including size, shape, polarity, and topological characteristics. They are widely used in QSAR modeling and virtual screening applications due to their comprehensive nature. from rdkit import Chem from rdkit.Chem import Descriptors # Define a function to calculate all the possible descriptors def calculate_descriptors(smiles, idx): mol = Chem.MolFromSmiles(smiles) try: mol = optimize_3d(mol, &#39;LOPT&#39;) except: pass desc_lst = [] for descriptor_name, descriptor_function in Descriptors.descList: try: descriptor_value = descriptor_function(mol) if descriptor_value == pd.notnull: print(f&#39;No value for {descriptor_name}, output: {descriptor_value} for {idx}:{smiles}&#39;) desc_lst.append(np.nan) else: desc_lst.append(descriptor_value) except: pass return desc_lst descriptor_list = [] for idx, sml in enumerate(tqdm(smiles)): compound_desc = calculate_descriptors(sml, idx) descriptor_list.append(compound_desc) rdkit_desc_df = pd.DataFrame(descriptor_list, columns=[name for name, _ in Descriptors.descList]).astype(np.float64) rdkit_desc_df = rdkit_desc_df.dropna(axis=1) rdkit_desc = rdkit_desc_df.values.astype(np.float64) Finally, Mordred descriptors provide a vast array of over 1600 three-dimensional, two-dimensional, and one-dimensional descriptors. These descriptors represent a wide variety of chemical information, ranging from simple atom counts and molecular weight to more complex descriptors such as electrotopological state indices and autocorrelation descriptors. The rich information provided by Mordred descriptors makes them an excellent choice for modeling complex molecular behaviors. from mordred import Calculator, descriptors molecules = [Chem.MolFromSmiles(sml) for sml in smiles] # Define a function to calculate all the possible descriptors def calculate_mordred_descriptors(mol, optimization= &#39;LOPT&#39;): mol = optimize_3d(mol, optimization) # Create a Mordred calculator object calculator = Calculator(descriptors) return calculator(mol) mdrd_descriptor_list = [] for idx, mol in enumerate(tqdm(molecules)): mol_desc = calculate_mordred_descriptors(mol) desc = list(mol_desc.asdict().values()) mdrd_descriptor_list.append(desc) mordred_desc_df = pd.DataFrame(mdrd_descriptor_list, columns=list(mol_desc.asdict().keys())).astype(np.float64) mordred_desc_df = mordred_desc_df.dropna(axis=1) mordred_desc = mordred_desc_df.values.astype(np.float64) To summarize, the featureization process leverages multiple molecular descriptors to capture molecular structures’ complexity and diversity. Each descriptor contributes unique information about the molecule, resulting in a comprehensive and informative representation. Featurizing Technique Description Size Binary 3D Information Adjustability MACCS fingerprints Predefined structural fragments 167 Yes No No Morgan fingerprints (Circular fingerprints) Hashing the environments of atoms in a molecule Adjustable (2048 in the example) Yes No Yes (Radius and length can be adjusted) PubChem fingerprints Chemical substructure or pattern 881 Yes No No Mol2Vec fingerprint SMILES as words Variable No No No RDKit descriptors Physico-chemical descriptors &gt;200 No Yes No Mordred descriptors Physico-chemical descriptors &gt;1600 No Yes No 2.2.3.2 Featurizing Proteins The transformation of protein sequences into embedding vectors using models pretrained on millions of proteins is highly beneficial. These models are trained to understand protein sequence patterns, structures, and dependencies. Thus, the resulting embeddings capture a wealth of information about protein sequences, including their evolutionary context, structural features, and biological functions. BioTransformers is a Python package that provides a unified API to use and evaluate several pre-trained models for protein sequences. These models transform protein sequences into meaningful numerical representations, also known as embeddings. The extracted embeddings can then be used in downstream machine learning tasks such as protein classification, clustering, or prediction of protein properties. The models you’ve selected, protbert and esm1_t34_670M_UR100, are two different pre-trained models available in BioTransformers. ProtBert is a transformer-based model trained on a large corpus of protein sequences using a masked language modeling objective, similar to BERT models in natural language processing. The model’s architecture enables it to capture complex patterns and dependencies in the sequence data. On the other hand, esm1_t34_670M_UR100 is part of the ESM (Evolutionary Scale Modeling) series of models, specifically trained on a large evolutionary scale of protein sequences. This model is designed to capture evolutionary patterns and sequence conservation information, which can be highly beneficial for protein-related tasks. from biotransformers import BioTransformers from tqdm.notebook import tqdm import torch import numpy as np import pandas as pd import pickle def compute_embeddings(bio_trans, sequences, batch_size=10): embeddings = np.empty((0,1024), float) for idx in tqdm(range(0, len(sequences), batch_size)): batch = sequences[idx:idx+batch_size] embd = bio_trans.compute_embeddings(batch, pool_mode=&#39;mean&#39;, batch_size=batch_size, silent=True)[&#39;mean&#39;] embeddings = np.vstack((embeddings, embd)) return embeddings def save_embeddings(embeddings, filename): with open(filename, &quot;wb&quot;) as f: pickle.dump(embeddings, f) # Load sequences seq_df = pd.read_csv(&#39;sequence.tsv&#39;, sep=&#39;\\t&#39;) sequences = list(seq_df[&#39;sequence&#39;]) # Backends backends = [&quot;protbert&quot;, &quot;esm1_t34_670M_UR100&quot;] for backend in backends: # Clear GPU memory torch.cuda.empty_cache() print(f&quot;Processing with backend: {backend}&quot;) bio_trans = BioTransformers(backend, num_gpus=1) embeddings = compute_embeddings(bio_trans, sequences) save_embeddings(embeddings, f&quot;{backend}_embeddings.pkl&quot;) This rich representation can be leveraged in downstream tasks, improving the performance of various bioinformatics applications such as protein function prediction, protein-protein interaction prediction, and many others. By using pre-trained embeddings, one can also significantly reduce the computational cost and complexity associated with training deep learning models from scratch on large protein datasets. 2.2.3.3 Featurizing Pathways BERT tokenizer from the Hugging Face Transformers library, a pre-trained model, was chosen for its ability to perform natural language processing tasks such as tokenization to vectorize biological pathways. This process involved splitting the text into individual tokens and encoding them as numerical IDs that could be understood by the BERT model. Padding and truncation techniques were applied to ensure consistent sequence lengths during tokenization. This step was critical as pathway descriptions often varied in length. By padding shorter sequences and truncating longer ones, uniformity was achieved. This would simply allow the model recognize different pathways from each other. from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;) string_list = pathway_df.select(pl.col(&#39;Wiki Pathway&#39;)).to_series().to_list() # Tokenize the pathway descriptions tokenized_strings = tokenizer(string_list, padding=True, truncation=True, return_tensors=&#39;pt&#39;) # Retrieve the tokenized input IDs pw_vector = tokenized_strings[&#39;input_ids&#39;] 2.2.4 Building COVID-19 Bio-Graph The datasets were further split into training, validation, and testing sets using a stratified approach to maintain a uniform distribution of classes across all sets. This enabled the creation of robust machine learning models capable of effectively learning from training data and generalizing to unseen data. The final output of this process was a HeteroData object from the PyTorch Geometric library, which represents a heterogeneous graph with various types of nodes and edges. This graph-based representation of the data encapsulates the interconnected nature of the compounds, proteins, and pathways, thereby providing a comprehensive overview of the interactions and associations within the COVID-19 cell profiling data. "],["results.html", "3 Results 3.1 About Bookdown", " 3 Results This is a sample book written using Bookdown. In this book, we’ll explore the power of combining R Markdown with Bookdown to create a beautifully formatted online book. 3.1 About Bookdown Bookdown is an R package that allows you to write books, articles, and reports in R Markdown. It supports multiple output formats, including HTML, PDF, and EPUB. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
